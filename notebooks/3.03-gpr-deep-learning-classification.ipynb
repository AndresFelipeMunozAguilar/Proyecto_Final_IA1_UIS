{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjq5aYppPwR"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "# <font color='#ECA702'>**Análisis Multimodal de Síntomas en la Enfermedad de Parkinson**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "89BxmLrSpPwT"
      },
      "outputs": [],
      "source": [
        "#@title **Importar librerías**\n",
        "\n",
        "# Manipulación de datos\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Pytorch essentials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# \n",
        "\n",
        "# Pytorch essentials for datasets.\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "# Utils\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import GroupKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kyzPDeVpPwU",
        "outputId": "fcb0f7b2-6c6e-4dec-b5e3-1fc11be9da49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1XRruCPRodR4OhRCRZYKsyODqa_GuBkfC/data_parkinson\n"
          ]
        }
      ],
      "source": [
        "#@title **Cargamos los datos desde Drive**\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/data_parkinson')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **Extraer el zip**\n",
        "\n",
        "target_path = os.path.join('../../../..') # Aquí es donde van a quedar los datos\n",
        "# deben quedar fuera de drive para poder acceder a ellos 'localmente'\n",
        "\n",
        "# Unzip the downloaded files\n",
        "zip_file_path = os.path.join('AudioVisualData_v7.zip')\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_path)\n",
        "\n",
        "%cd ../../../.. # Me muevo hasta donde quedaron los datos\n",
        "%ls # Verifico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI_cmB-BpPwU"
      },
      "source": [
        "## <font color='#4C5FDA'>**Procesamiento del dataset**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0Bee8VqyYi"
      },
      "source": [
        "### <font color='#52F17F'>**Creación del Dataset Pandas auxiliar**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "2mB_EpmMqxm-",
        "outputId": "6394332c-d882-46e5-b59f-c6dc48728a89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio_path</th>\n",
              "      <th>frames_path</th>\n",
              "      <th>label</th>\n",
              "      <th>flatten_log_mel_spectogram</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>patient</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.66636657714844, -19.715749740600586, -24....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-61.8838005065918, -28.279430389404297, -28.8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-59.091400146484375, -25.001014709472656, -28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.556915283203125, -20.145050048828125, -20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-69.82708740234375, -20.313282012939453, -26....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                audio_path  \\\n",
              "patient                                                      \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "\n",
              "                                               frames_path  label  \\\n",
              "patient                                                             \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "\n",
              "                                flatten_log_mel_spectogram  \n",
              "patient                                                     \n",
              "C2       [-60.66636657714844, -19.715749740600586, -24....  \n",
              "C2       [-61.8838005065918, -28.279430389404297, -28.8...  \n",
              "C2       [-59.091400146484375, -25.001014709472656, -28...  \n",
              "C2       [-60.556915283203125, -20.145050048828125, -20...  \n",
              "C2       [-69.82708740234375, -20.313282012939453, -26....  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"../data/data.csv\", index_col='patient')\n",
        "\n",
        "def string2array(string: str):\n",
        "  \"\"\" Convierte un string con datos a numpy array \"\"\"\n",
        "  string = string.strip('[]')\n",
        "  return np.fromstring(string, sep=',')\n",
        "\n",
        "data['flatten_log_mel_spectogram'] = data['flatten_log_mel_spectogram'].apply(string2array)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_gRyCKHpPwZ"
      },
      "source": [
        "### <font color='#52F17F'>**Creación del Dataset PyTorch**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_kEXiniXpPwV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Constante sacada de la media de cantidad de frames de todos los vídeos multiplicada por 0.1\n",
        "para tener el 10%\n",
        "\"\"\"\n",
        "\n",
        "FRAMES_PROMEDIO = 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8OL_3J7jpPwZ"
      },
      "outputs": [],
      "source": [
        "class PatientDataset(Dataset):\n",
        "  def __init__(self, dataframe, n_frames = FRAMES_PROMEDIO):\n",
        "    self.dataframe = dataframe\n",
        "    # self.transform = transform\n",
        "    self.n_frames = n_frames\n",
        "    # self.audios = sorted(os.listdir(audio_dir)) # Listamos todas las imagenes en la carpeta\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    \"\"\" Carga de la imagen \"\"\"\n",
        "\n",
        "    # Entramos a la carpeta y conseguimos el audio de la lista\n",
        "    # audio_path = os.path.join(self.audio_dir, self.audios[index])\n",
        "    # audio_path = os.path.join(self.dataframe['audio_path'][index])\n",
        "\n",
        "    # Leemos el audio\n",
        "    audio = self.dataframe['flatten_log_mel_spectogram'][index].astype(np.float32)\n",
        "    audio = audio.reshape((90, 10))\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0)\n",
        "\n",
        "    \"\"\" Carga de los frames \"\"\"\n",
        "\n",
        "    # Entramos a la carpeta y conseguimos los frames de la lista.\n",
        "      # La razon por la que uso la misma lista de audios es porque el audio y el vídeo\n",
        "      # tienen el mismo nombre, solo cambia la ruta de la carpeta y la extensión.\n",
        "    \"\"\" audios: AudioVisualData_v7/Control/C0/Phonemes/audio/C0-1-Pakata.mp3 \"\"\"\n",
        "    \"\"\" vídeo: AudioVisualData_v7/Control/C0/Phonemes/frames/C0-1-Pakata.mov \"\"\"\n",
        "    video_path = os.path.join(self.dataframe['frames_path'][index])\n",
        "    # video_path = os.path.join(self.video_dir, self.audios[index].replace(\".mp3\", \".mov\"))\n",
        "\n",
        "    frames_path = sorted(os.listdir(video_path))[:self.n_frames]\n",
        "    frames = []\n",
        "    for frame in frames_path:\n",
        "      frame_path = os.path.join(self.dataframe['frames_path'][index], frame)\n",
        "      # print(frame_path)\n",
        "      frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n",
        "      frame = cv2.resize(frame, (112, 112)) # Resize a 112x112\n",
        "      frame = frame / 255. # Normalizamos entre 0 y 1\n",
        "      frames.append(frame)\n",
        "\n",
        "    frames = np.expand_dims(np.stack(frames), axis=1).astype('float32')\n",
        "    frames = frames.transpose(1, 0, 2, 3)\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "    \"\"\" Consiguiendo el label \"\"\"\n",
        "\n",
        "    # Para conseguir la etiqueta simplemente me tengo que fijar en la ruta. Si es 'Parkinson' 1\n",
        "    # sino 0.\n",
        "\n",
        "    label = 1 if 'Parkinson' in self.dataframe['audio_path'][index] else 0\n",
        "\n",
        "    return audio, frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hYyG5ZSpPwa",
        "outputId": "ea5c385a-82af-443a-b94b-36e8bbfd8975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1091\n",
            "torch.Size([1, 90, 10]) torch.Size([1, 14, 112, 112]) 0\n"
          ]
        }
      ],
      "source": [
        "complete_dataset = PatientDataset(data)\n",
        "print(complete_dataset.__len__())\n",
        "print(complete_dataset[0][0].shape, complete_dataset[0][1].shape , complete_dataset[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='#52F17F'>**Visualización de los datos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_frame_strip(frames, rows=2, cols=7):\n",
        "    \"\"\"\n",
        "    Hace un plot de la tira de frames en una matriz de imágenes.\n",
        "\n",
        "    Args:\n",
        "        frames (np.ndarray): Tensor de frames con forma (num_frames, height, width, channels).\n",
        "        rows (int): Número de filas en la tira de frames.\n",
        "        cols (int): Número de columnas en la tira de frames.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 6))\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            idx = i * cols + j\n",
        "            if idx < len(frames):\n",
        "                axes[i, j].imshow(frames[idx], cmap='gray')\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].set_title(f\"Frame: {idx}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_frame_strip(complete_dataset[0][1].squeeze(0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 10)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complete_dataset[0][0].squeeze(0).numpy().reshape((90, 10)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "librosa.display.specshow(complete_dataset[0][0].squeeze(0).numpy(), y_axis='mel', x_axis='time', sr=48000)\n",
        "plt.colorbar(format=\"%+2.f dB\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3z4R6jpPwb"
      },
      "source": [
        "### <font color='#52F17F'>**Partición de los datos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZT6iQ5vspPwb"
      },
      "outputs": [],
      "source": [
        "# Extraer los datos para GroupKFold https://discuss.pytorch.org/t/custom-datatype-for-many-images-to-one-label/87629\n",
        "X = [i for i in range(len(data))]\n",
        "y = data['label'].values\n",
        "groups = data.index.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1R5D3F0pPwb",
        "outputId": "aba89d37-0c23-4793-96ea-605880458044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "División 1:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P7']\n",
            "  Pacientes en prueba: ['C5' 'P8']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 2:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P8']\n",
            "  Pacientes en prueba: ['C4' 'P7']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 3:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C3' 'P6']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 4:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C2' 'P5']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 5:\n",
            "  Pacientes en entrenamiento: ['C0' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C1' 'P4']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 6:\n",
            "  Pacientes en entrenamiento: ['C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C0' 'P1']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 7:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C5' 'P1' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C6' 'P0']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "groupk_folds = 7\n",
        "gkf = GroupKFold(n_splits=groupk_folds)\n",
        "# Realizar la validación cruzada por grupos\n",
        "for i, (train_index, test_index) in enumerate(gkf.split(X, y, groups), 1):\n",
        "    train_groups = groups[train_index]\n",
        "    test_groups = groups[test_index]\n",
        "\n",
        "    print(f\"División {i}:\")\n",
        "    print(\"  Pacientes en entrenamiento:\", np.unique(train_groups))\n",
        "    print(\"  Pacientes en prueba:\", np.unique(test_groups))\n",
        "    print(\"  Número de pacientes en entrenamiento:\", len(np.unique(train_groups)))\n",
        "    print(\"  Número de pacientes en prueba:\", len(np.unique(test_groups)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVi02_rZpPwb"
      },
      "source": [
        "## <font color='#ECA702'>**Modelo CNN**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irytALTWpPwb"
      },
      "source": [
        "### <font color='#52F17F'>**Creando el modelo**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "VZiQoJ_rpPwb"
      },
      "outputs": [],
      "source": [
        "class AudioModule(nn.Module):\n",
        "  def __init__(self, n_channels, n_classes):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(n_channels, 32, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(64, 32)\n",
        "    self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.gap(x)\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjJuKihMpPwc",
        "outputId": "d426437b-6788-49bb-cc15-e34db0dd7d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada: torch.Size([1, 1, 90, 10])\n",
            "Salida: torch.Size([1, 32])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "n_channels = 1\n",
        "n_classes = 2\n",
        "input_audio = complete_dataset[0][0].unsqueeze(0)\n",
        "print(f\"Entrada: {input_audio.size()}\")\n",
        "\n",
        "model = AudioModule(n_channels=n_channels, n_classes=n_classes)\n",
        "ouput = model(input_audio)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pkgvMWDZpPwc"
      },
      "outputs": [],
      "source": [
        "class VideoModule(nn.Module):\n",
        "  def __init__(self, n_channels, n_classes):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv3d(n_channels, 32, 3, padding=1)\n",
        "    self.conv2 = nn.Conv3d(32, 64, 3, padding=1)\n",
        "    self.pool = nn.MaxPool3d(2)\n",
        "    self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "    self.fc1 = nn.Linear(64 * 7, 32)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.gap(x)\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZirEFYHUpPwc",
        "outputId": "eb01a580-0b13-4910-8f76-2aee11c58053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada: (torch.Size([1, 1, 14, 112, 112]), torch.float32)\n",
            "Salida: torch.Size([1, 32])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "n_channels = 1\n",
        "n_classes = 2\n",
        "input_video = complete_dataset[0][1].unsqueeze(0)\n",
        "print(f\"Entrada: {input_video.size(), input_video.dtype}\")\n",
        "\n",
        "model = VideoModule(n_channels=n_channels, n_classes=n_classes)\n",
        "ouput = model(input_video)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6Safw_JEpPwc"
      },
      "outputs": [],
      "source": [
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super().__init__()\n",
        "        self.audio_model = AudioModule(n_channels, n_classes)\n",
        "        self.video_model = VideoModule(n_channels, n_classes)\n",
        "        self.fc1 = nn.Linear(32 * 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)  # Suponiendo que es una tarea de clasificación binaria\n",
        "\n",
        "    def forward(self, audio, video):\n",
        "        audio_features = self.audio_model(audio)\n",
        "        video_features = self.video_model(video)\n",
        "        combined = torch.cat((audio_features, video_features), dim=1)\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHuZqJhpPwc",
        "outputId": "962882dd-f183-4f73-a2c9-7257ba9e2e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada audio: (torch.Size([1, 1, 90, 10]), torch.float32)\n",
            "Entrada video: (torch.Size([1, 1, 14, 112, 112]), torch.float32)\n",
            "Salida: torch.Size([1, 2])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "n_channels = 1\n",
        "n_classes = 2\n",
        "input_audio = complete_dataset[0][0].unsqueeze(0)\n",
        "input_video = complete_dataset[0][1].unsqueeze(0)\n",
        "print(f\"Entrada audio: {input_audio.size(), input_audio.dtype}\")\n",
        "print(f\"Entrada video: {input_video.size(), input_video.dtype}\")\n",
        "\n",
        "model = MultimodalModel(n_channels=n_channels, n_classes=n_classes)\n",
        "ouput = model(input_audio, input_video)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Oqu4j5pPwd"
      },
      "source": [
        "### <font color='#52F17F'>**Entrenamiento del modelo**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "njKMV7L_pPwd"
      },
      "outputs": [],
      "source": [
        "# Hiperparametros\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-WHkwzRDpPwd"
      },
      "outputs": [],
      "source": [
        "# For fold results\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "lFRWKFphpPwd",
        "outputId": "7372800b-1e5a-4375-8cda-df970441d471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOLD 1\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/935 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 204/935 [00:10<00:36, 20.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/935 [00:20<00:26, 19.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 602/935 [00:30<00:16, 19.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 802/935 [00:40<00:06, 19.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 1: 50 %\n",
            "--------------------------------\n",
            "FOLD 2\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 203/935 [00:10<00:37, 19.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/935 [00:20<00:26, 19.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 603/935 [00:30<00:16, 19.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 803/935 [00:40<00:06, 19.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 2: 50 %\n",
            "--------------------------------\n",
            "FOLD 3\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 202/935 [00:10<00:37, 19.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 402/935 [00:20<00:27, 19.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 602/935 [00:30<00:16, 19.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 802/935 [00:41<00:06, 19.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.680\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 3: 7 %\n",
            "--------------------------------\n",
            "FOLD 4\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 203/935 [00:10<00:37, 19.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.710\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/935 [00:20<00:27, 19.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 603/935 [00:30<00:16, 19.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 803/935 [00:41<00:06, 19.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 4: 50 %\n",
            "--------------------------------\n",
            "FOLD 5\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 203/935 [00:10<00:37, 19.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/935 [00:20<00:27, 19.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 603/935 [00:30<00:16, 19.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 803/935 [00:41<00:06, 19.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 5: 50 %\n",
            "--------------------------------\n",
            "FOLD 6\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 203/935 [00:10<00:37, 19.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/935 [00:20<00:27, 19.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 603/935 [00:30<00:16, 19.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 803/935 [00:41<00:06, 19.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 935/935 [00:47<00:00, 19.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 6: 48 %\n",
            "--------------------------------\n",
            "FOLD 7\n",
            "--------------------------------\n",
            "EPOCH 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 203/936 [00:10<00:37, 19.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   200 loss: 0.708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 403/936 [00:20<00:27, 19.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   400 loss: 0.696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 603/936 [00:30<00:17, 19.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   600 loss: 0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 803/936 [00:41<00:06, 19.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch   800 loss: 0.695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 936/936 [00:47<00:00, 19.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training process has finished. Saving trained model.\n",
            "Starting testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 7: 49 %\n",
            "--------------------------------\n",
            "GroupK-FOLD CROSS VALIDATION RESULTS FOR 7 FOLDS\n",
            "--------------------------------\n",
            "Fold 0: 50.0 %\n",
            "Fold 1: 50.641025641025635 %\n",
            "Fold 2: 7.051282051282051 %\n",
            "Fold 3: 50.0 %\n",
            "Fold 4: 50.0 %\n",
            "Fold 5: 48.717948717948715 %\n",
            "Fold 6: 49.67741935483871 %\n",
            "Average: 43.72681082358502 %\n"
          ]
        }
      ],
      "source": [
        "# Loop encontrado y modificado de: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\n",
        "for fold, (train_ids, test_ids) in enumerate(gkf.split(X, y, groups)):\n",
        "\n",
        "  # Print\n",
        "  print(f'FOLD {fold+1}')\n",
        "  print('--------------------------------')\n",
        "\n",
        "  # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.Subset(complete_dataset, train_ids)\n",
        "  test_subsampler = torch.utils.data.Subset(complete_dataset, test_ids)\n",
        "\n",
        "  # Define data loaders for training and testing data in this fold\n",
        "  trainloader = DataLoader(train_subsampler, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "  testloader = DataLoader(test_subsampler, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "  \"\"\" Parametros del modelo \"\"\"\n",
        "\n",
        "  # Creacion del modelo y lo movemos a la GPU o CPU\n",
        "  model = MultimodalModel(n_channels=1, n_classes=2)\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  # Funcion de perdida (loss)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Algoritmos de optimización del gradiente\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  epoch_number = 0\n",
        "\n",
        "  EPOCHS = 1\n",
        "\n",
        "  for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "      print(f\"EPOCH {epoch_number + 1}\")\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(tqdm(trainloader, 0)):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          audios, videos, labels = data\n",
        "\n",
        "          # Movemos los datos al device\n",
        "          audios = audios.to(DEVICE)\n",
        "          videos = videos.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(audios, videos)\n",
        "          # print(ouputs.dtype)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 200 == 199:    # print every 100 mini-batches\n",
        "              print(f'batch {i + 1:5d} loss: {running_loss / 200:.3f}')\n",
        "              running_loss = 0.0\n",
        "      epoch_number += 1 # Pasamos a la siguiente época\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished. Saving trained model.')\n",
        "\n",
        "  # Print about testing\n",
        "  print('Starting testing')\n",
        "\n",
        "  # Saving the model\n",
        "  save_path = f'../models/model-fold-{fold}.pth'\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "\n",
        "  # Evaluationfor this fold\n",
        "  correct, total = 0, 0\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterate over the test data and generate predictions\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      audios, videos, labels = data\n",
        "\n",
        "      # Movemos los datos al device\n",
        "      audios = audios.to(DEVICE)\n",
        "      videos = videos.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Generate outputs\n",
        "      outputs = model(audios, videos)\n",
        "\n",
        "      # Set total and correct\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print accuracy\n",
        "    print('Accuracy for fold %d: %d %%' % (fold+1, 100.0 * correct / total))\n",
        "    print('--------------------------------')\n",
        "    results[fold] = 100.0 * (correct / total)\n",
        "\n",
        "# Print fold results\n",
        "print(f'GroupK-FOLD CROSS VALIDATION RESULTS FOR {groupk_folds} FOLDS')\n",
        "print('--------------------------------')\n",
        "sum = 0.0\n",
        "for key, value in results.items():\n",
        "  print(f'Fold {key}: {value} %')\n",
        "  sum += value\n",
        "print(f'Average: {sum/len(results.items())} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JINh8fFAu1Rv"
      },
      "source": [
        "## <font color='#4C5FDA'>**Anexos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KDI_GmFYpPwZ"
      },
      "outputs": [],
      "source": [
        "CONTROL_PATH = \"AudioVisualData_v7/Control\"\n",
        "PARKINSON_PATH = \"AudioVisualData_v7/Parkinson\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3a-_K8epPwa",
        "outputId": "0fe8eb49-48e1-4c7b-9905-5f4b9432d967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de pacientes de control: 7.0\n",
            "Cantidad de audios de control: 546\n",
            "Cantidad de frames de control: 7644\n",
            "\n",
            "Cantidad de pacientes de parkinson: 7.0\n",
            "Cantidad de audios de parkinson: 545\n",
            "Cantidad de frames de parkinson: 7630\n"
          ]
        }
      ],
      "source": [
        "# Primero consigo la ruta de videos y audios para cada uno de los pacientes\n",
        "\n",
        "# groups = []\n",
        "\n",
        "dirs_control = []\n",
        "control_registros = 0\n",
        "for patient in os.listdir(CONTROL_PATH):\n",
        "  # groups.append(patient)\n",
        "  # print(patient)\n",
        "  for record_type in os.listdir(os.path.join(CONTROL_PATH, patient)):\n",
        "    # print(os.path.join(CONTROL_PATH, patient, record_type))\n",
        "    dirs_control.append(os.path.join(CONTROL_PATH, patient, record_type))\n",
        "    _, _, files = next(os.walk(os.path.join(CONTROL_PATH, patient, record_type, \"audio\")))\n",
        "    # print(files)\n",
        "    control_registros += len(files)\n",
        "\n",
        "dirs_parkinson = []\n",
        "parkinson_registros = 0\n",
        "for patient in os.listdir(PARKINSON_PATH):\n",
        "  # groups.append(patient)\n",
        "  # print(patient)\n",
        "  for record_type in os.listdir(os.path.join(PARKINSON_PATH, patient)):\n",
        "    #   print(os.path.join(PARKINSON_PATH, patient, record_type))\n",
        "    dirs_parkinson.append(os.path.join(PARKINSON_PATH, patient, record_type))\n",
        "    _, _, files = next(os.walk(os.path.join(PARKINSON_PATH, patient, record_type, \"audio\")))\n",
        "    parkinson_registros += len(files)\n",
        "\n",
        "# Imprimo la cantidad de pacientes y datos que tenemos\n",
        "print(f\"Cantidad de pacientes de control: {len(dirs_control)/3}\")\n",
        "print(f\"Cantidad de audios de control: {control_registros}\")\n",
        "print(f\"Cantidad de frames de control: {control_registros*14}\")\n",
        "print()\n",
        "print(f\"Cantidad de pacientes de parkinson: {len(dirs_parkinson)/3}\")\n",
        "print(f\"Cantidad de audios de parkinson: {parkinson_registros}\")\n",
        "print(f\"Cantidad de frames de parkinson: {parkinson_registros*14}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZu7CgYHpPwa"
      },
      "outputs": [],
      "source": [
        "control_datasets = []\n",
        "for patient in dirs_control:\n",
        "  folders = sorted(os.listdir(patient))\n",
        "  # print(folders)\n",
        "  control_datasets.append(PatientDataset(data,f\"{patient}/{folders[0]}\", f\"{patient}/{folders[1]}\"))\n",
        "\n",
        "parkinson_datasets = []\n",
        "for patient in dirs_parkinson:\n",
        "  folders = sorted(os.listdir(patient))\n",
        "  # print(folders)\n",
        "  parkinson_datasets.append(PatientDataset(data,f\"{patient}/{folders[0]}\", f\"{patient}/{folders[1]}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDpnci2npPwa"
      },
      "outputs": [],
      "source": [
        "# Datasets (concatenate) https://stackoverflow.com/questions/60840500/pytorch-concatenating-datasets-before-using-dataloader\n",
        "control_dataset = torch.utils.data.ConcatDataset(control_datasets)\n",
        "parkinson_dataset = torch.utils.data.ConcatDataset(parkinson_datasets)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
