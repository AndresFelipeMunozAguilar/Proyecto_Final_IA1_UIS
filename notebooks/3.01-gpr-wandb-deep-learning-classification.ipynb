{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjq5aYppPwR"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "# <font color='#ECA702'>**AnÃ¡lisis Multimodal de SÃ­ntomas en la Enfermedad de Parkinson**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **Instalar paquetes necesarios**\n",
        "%%capture \n",
        "! pip install torchmetrics\n",
        "! pip install torchinfo\n",
        "! pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "89BxmLrSpPwT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.12.1-git20200711.33e2d80-dfsg1-0.6 is an invalid version and will not be supported in a future release\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#@title **Importar librerÃ­as**\n",
        "\n",
        "# ManipulaciÃ³n de datos\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Pytorch essentials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "# Pytorch essentials for datasets.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Pytorch metrics\n",
        "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, BinaryRecall, BinaryPrecision\n",
        "\n",
        "# Utils\n",
        "import matplotlib.pyplot as plt\n",
        "# import copy\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kyzPDeVpPwU",
        "outputId": "942fc83c-f610-4bdb-a882-65fa126bc7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1XRruCPRodR4OhRCRZYKsyODqa_GuBkfC/data_parkinson\n"
          ]
        }
      ],
      "source": [
        "#@title **Cargamos los datos desde Drive**\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/data_parkinson')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqX7Xwgd-TuU",
        "outputId": "d7ffd662-8297-48d4-d8a6-13cab5955389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "\u001b[0m\u001b[01;34mAudioVisualData_v7\u001b[0m/  data.csv\n"
          ]
        }
      ],
      "source": [
        "#@title **Extraer el zip**\n",
        "\n",
        "os.mkdir('../../../../data')\n",
        "os.popen('cp data.csv ../../../../data/data.csv')\n",
        "target_path = os.path.join('../../../../data') # AquÃ­ es donde van a quedar los datos\n",
        "# deben quedar fuera de drive para poder acceder a ellos 'localmente'\n",
        "\n",
        "# Unzip the downloaded files\n",
        "zip_file_path = os.path.join('AudioVisualData_v7.zip')\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_path)\n",
        "\n",
        "# # Me muevo hasta donde quedaron los datos\n",
        "%cd ../../../../data\n",
        "# Verifico\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI_cmB-BpPwU"
      },
      "source": [
        "# <font color='#4C5FDA'>**1. Procesamiento del dataset**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0Bee8VqyYi"
      },
      "source": [
        "## <font color='#52F17F'>**CreaciÃ³n del Dataset Pandas auxiliar**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "2mB_EpmMqxm-",
        "outputId": "44908149-391d-4822-a101-05fcfa878226"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio_path</th>\n",
              "      <th>frames_path</th>\n",
              "      <th>label</th>\n",
              "      <th>flatten_log_mel_spectogram</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>patient</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.66636657714844, -19.715749740600586, -24....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-61.8838005065918, -28.279430389404297, -28.8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-59.091400146484375, -25.001014709472656, -28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.556915283203125, -20.145050048828125, -20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-69.82708740234375, -20.313282012939453, -26....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                audio_path  \\\n",
              "patient                                                      \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "\n",
              "                                               frames_path  label  \\\n",
              "patient                                                             \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "\n",
              "                                flatten_log_mel_spectogram  \n",
              "patient                                                     \n",
              "C2       [-60.66636657714844, -19.715749740600586, -24....  \n",
              "C2       [-61.8838005065918, -28.279430389404297, -28.8...  \n",
              "C2       [-59.091400146484375, -25.001014709472656, -28...  \n",
              "C2       [-60.556915283203125, -20.145050048828125, -20...  \n",
              "C2       [-69.82708740234375, -20.313282012939453, -26....  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"data.csv\", index_col='patient')\n",
        "\n",
        "def string2array(string: str):\n",
        "  \"\"\" Convierte un string con datos a numpy array \"\"\"\n",
        "  string = string.strip('[]')\n",
        "  return np.fromstring(string, sep=',')\n",
        "\n",
        "data['flatten_log_mel_spectogram'] = data['flatten_log_mel_spectogram'].apply(string2array)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_gRyCKHpPwZ"
      },
      "source": [
        "## <font color='#52F17F'>**CreaciÃ³n del Dataset PyTorch**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8OL_3J7jpPwZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Constante sacada de la media de cantidad de frames de todos los vÃ­deos multiplicada por 0.1\n",
        "para tener el 10%\n",
        "\"\"\"\n",
        "\n",
        "FRAMES_PROMEDIO = 14\n",
        "\n",
        "class PatientDataset(Dataset):\n",
        "  def __init__(self, dataframe, n_frames = FRAMES_PROMEDIO):\n",
        "    self.dataframe = dataframe\n",
        "    # self.transform = transform\n",
        "    self.n_frames = n_frames\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    \"\"\" Carga de la imagen \"\"\"\n",
        "\n",
        "    # Leemos el audio\n",
        "    audio = self.dataframe['flatten_log_mel_spectogram'][index].astype(np.float32)\n",
        "    audio = audio.reshape((90, 10))\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0)\n",
        "\n",
        "    \"\"\" Carga de los frames \"\"\"\n",
        "\n",
        "    video_path = os.path.join(self.dataframe['frames_path'][index])\n",
        "\n",
        "    frames_path = sorted(os.listdir(video_path))[:self.n_frames]\n",
        "    frames = []\n",
        "    for frame in frames_path:\n",
        "      frame_path = os.path.join(self.dataframe['frames_path'][index], frame)\n",
        "      # print(frame_path)\n",
        "      frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n",
        "      frame = cv2.resize(frame, (112, 112)) # Resize a 112x112\n",
        "      frame = frame / 255. # Normalizamos entre 0 y 1\n",
        "      frames.append(frame)\n",
        "\n",
        "    frames = np.expand_dims(np.stack(frames), axis=1).astype('float32')\n",
        "    frames = frames.transpose(1, 0, 2, 3)\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "    \"\"\" Consiguiendo el label \"\"\"\n",
        "\n",
        "    # Para conseguir la etiqueta simplemente me tengo que fijar en la ruta. Si es 'Parkinson' 1\n",
        "    # sino 0.\n",
        "\n",
        "    label = 1 if 'Parkinson' in self.dataframe['audio_path'][index] else 0\n",
        "\n",
        "    return audio, frames, label\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hYyG5ZSpPwa",
        "outputId": "a20701d4-968c-41c6-d21e-478e6e27fd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1091\n",
            "torch.Size([1, 90, 10]) torch.Size([1, 14, 112, 112]) 0\n"
          ]
        }
      ],
      "source": [
        "complete_dataset = PatientDataset(data)\n",
        "print(complete_dataset.__len__())\n",
        "print(complete_dataset[0][0].shape, complete_dataset[0][1].shape , complete_dataset[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3z4R6jpPwb"
      },
      "source": [
        "## <font color='#52F17F'>**ParticiÃ³n de los datos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DivisiÃ³n 1:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P7']\n",
            "  Pacientes en prueba: ['C5' 'P8']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 2:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P8']\n",
            "  Pacientes en prueba: ['C4' 'P7']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 3:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C3' 'P6']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 4:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C2' 'P5']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 5:\n",
            "  Pacientes en entrenamiento: ['C0' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C1' 'P4']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 6:\n",
            "  Pacientes en entrenamiento: ['C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C0' 'P1']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n",
            "DivisiÃ³n 7:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C5' 'P1' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C6' 'P0']\n",
            "  NÃºmero de pacientes en entrenamiento: 12\n",
            "  NÃºmero de pacientes en prueba: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extraer los datos para GroupKFold\n",
        "X = [i for i in range(len(data))]\n",
        "y = data['label'].values\n",
        "groups = data.index.values\n",
        "\n",
        "groupk_folds = 7\n",
        "gkf = GroupKFold(n_splits=groupk_folds)\n",
        "\n",
        "# Realizar la validaciÃ³n cruzada por grupos\n",
        "for i, (train_index, test_index) in enumerate(gkf.split(X, y, groups), 1):\n",
        "    train_groups = groups[train_index]\n",
        "    test_groups = groups[test_index]\n",
        "\n",
        "    print(f\"DivisiÃ³n {i}:\")\n",
        "    print(\"  Pacientes en entrenamiento:\", np.unique(train_groups))\n",
        "    print(\"  Pacientes en prueba:\", np.unique(test_groups))\n",
        "    print(\"  NÃºmero de pacientes en entrenamiento:\", len(np.unique(train_groups)))\n",
        "    print(\"  NÃºmero de pacientes en prueba:\", len(np.unique(test_groups)))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_folds(data:pd.DataFrame):\n",
        "    # Extraer los datos para GroupKFold\n",
        "    X = np.array([i for i in range(len(data))])\n",
        "    y = data['label'].values\n",
        "    groups = data.index.values\n",
        "\n",
        "    folds_dict = {}\n",
        "    groupk_folds = 7\n",
        "    gkf = GroupKFold(n_splits=groupk_folds)\n",
        "\n",
        "    # Realizar la validaciÃ³n cruzada por grupos\n",
        "    for i, (train_index, test_index) in enumerate(gkf.split(X, y, groups), 1):        \n",
        "        fold_name = f\"fold_{i}\"\n",
        "        folds_dict[fold_name] = {\n",
        "            'train': train_index,\n",
        "            'test': test_index\n",
        "        }\n",
        "\n",
        "    return folds_dict\n",
        "\n",
        "def make_subdataframes(data:pd.DataFrame, folds:dict):\n",
        "  # Crear subdataframes\n",
        "  subdataframes = {}\n",
        "\n",
        "  for fold_name, indices in folds.items():\n",
        "      train_df = data.iloc[indices['train']]\n",
        "    #   val_df = data.iloc[indices['val']]\n",
        "      test_df = data.iloc[indices['test']]\n",
        "      \n",
        "      subdataframes[fold_name] = {\n",
        "          'train': train_df,\n",
        "        #   'val': val_df,\n",
        "          'test': test_df\n",
        "      }\n",
        "  \n",
        "  return subdataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(slice=1, fold:int=None):\n",
        "\n",
        "    # Generate folds\n",
        "    folds = make_folds(data)\n",
        "\n",
        "    # Create subdataframes  \n",
        "    subdataframes = make_subdataframes(data, folds)\n",
        "\n",
        "    if not fold:\n",
        "      fold = np.random.choice(range(1, 8))\n",
        "\n",
        "    fold_name = f'fold_{fold}'\n",
        "    print(f\"FOLD {fold}\\n-------------------------------\")\n",
        "\n",
        "    train_dataset = PatientDataset(subdataframes[fold_name]['train'])\n",
        "    test_dataset = PatientDataset(subdataframes[fold_name]['test'])\n",
        "    \n",
        "    # test with less data, it helped me to set up the experiments faster if slice=1\n",
        "    # then it returns the complete dataset\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, \n",
        "                                            indices=range(0, len(train_dataset), slice))\n",
        "    test_dataset = torch.utils.data.Subset(test_dataset, \n",
        "                                            indices=range(0, len(test_dataset), slice))\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True,\n",
        "                        pin_memory=True, num_workers=2)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVi02_rZpPwb"
      },
      "source": [
        "# <font color='#ECA702'>**2. Deep Learning ðŸ¤–**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='#EB9A54'>**Multimodal fully connected model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MiddleModule(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(in_features, out_features)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "class MultimodalModel(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear((1 * 14 * 112 * 112) + (1 * 90 * 10), 1024)\n",
        "    self.features_module = nn.Sequential(\n",
        "      MiddleModule(1024, 512),\n",
        "      MiddleModule(512, 256),\n",
        "      MiddleModule(256, 128),\n",
        "    )\n",
        "    self.fc4 = nn.Linear(128, n_classes)\n",
        "\n",
        "\n",
        "  def forward(self, audio, video):\n",
        "    # https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch\n",
        "    audio = audio.view(audio.size(0), -1) # Aplane los audios conservando el batch\n",
        "    # devuelve: # batch_size x (90*10)\n",
        "\n",
        "    video = video.view(video.size(0), -1) # Aplane los videos conservando el batch\n",
        "    # devuelve: # batch_size x (14*112*112)\n",
        "\n",
        "    combined = torch.cat((video, audio), dim=1) # https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "    x = F.relu(self.fc1(combined))\n",
        "    x = self.features_module(x)\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada audio: (torch.Size([2, 1, 90, 10]), torch.float32)\n",
            "Entrada video: (torch.Size([2, 1, 14, 112, 112]), torch.float32)\n",
            "Salida: torch.Size([2, 1])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "input_audio = torch.rand([2, 1, 90, 10])\n",
        "input_video = torch.rand([2, 1, 14, 112, 112])\n",
        "print(f\"Entrada audio: {input_audio.size(), input_audio.dtype}\")\n",
        "print(f\"Entrada video: {input_video.size(), input_video.dtype}\")\n",
        "\n",
        "model = MultimodalModel(n_classes=1)\n",
        "ouput = model(input_audio, input_video)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultimodalModel(\n",
            "  (fc1): Linear(in_features=176516, out_features=196, bias=True)\n",
            "  (features_module): Sequential(\n",
            "    (0): MiddleModule(\n",
            "      (fc): Linear(in_features=196, out_features=512, bias=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): MiddleModule(\n",
            "      (fc): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MultimodalModel                          [2, 1]                    --\n",
              "â”œâ”€Linear: 1-1                            [2, 196]                  34,597,332\n",
              "â”œâ”€Sequential: 1-2                        [2, 256]                  --\n",
              "â”‚    â””â”€MiddleModule: 2-1                 [2, 512]                  --\n",
              "â”‚    â”‚    â””â”€Linear: 3-1                  [2, 512]                  100,864\n",
              "â”‚    â”‚    â””â”€ReLU: 3-2                    [2, 512]                  --\n",
              "â”‚    â””â”€MiddleModule: 2-2                 [2, 256]                  --\n",
              "â”‚    â”‚    â””â”€Linear: 3-3                  [2, 256]                  131,328\n",
              "â”‚    â”‚    â””â”€ReLU: 3-4                    [2, 256]                  --\n",
              "â”œâ”€Linear: 1-3                            [2, 1]                    257\n",
              "==========================================================================================\n",
              "Total params: 34,829,781\n",
              "Trainable params: 34,829,781\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 69.66\n",
              "==========================================================================================\n",
              "Input size (MB): 1.41\n",
              "Forward/backward pass size (MB): 0.02\n",
              "Params size (MB): 139.32\n",
              "Estimated Total Size (MB): 140.75\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model, (input_video.size(), input_audio.size()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='#EB9A54'>**Utils functions and hyperparameters**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparametros\n",
        "LEARNING_RATE = 0.0001\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_log(loss, accuracy, step, current):\n",
        "    \"\"\" Log the metrics for the current batch into wandb\n",
        "\n",
        "    Args:\n",
        "        loss: the value of the loss at current batch\n",
        "        accuracy: the value of the accuracy at current batch\n",
        "        step: actual step\n",
        "        current: actual batch\n",
        "    \"\"\"\n",
        "\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"step\":step, \"train_loss\": loss, \"train_accuracy\": accuracy})\n",
        "    print(f\"train loss: {loss:.3f} accuracy: {accuracy:.3f} [after {current} batches]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **train batch**\n",
        "\n",
        "def train_batch(audios, videos, labels, model, optimizer, criterion, metrics_fn):\n",
        "  audios, videos, labels = audios.to(DEVICE), videos.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "  # Forward pass âž¡\n",
        "  outputs = model(audios, videos)\n",
        "  loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "  accuracy = metrics_fn(outputs, labels.unsqueeze(1).float())\n",
        "\n",
        "  # Backward pass â¬…\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  # Step with optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, accuracy_fn, epochs, losses=None):\n",
        "    # Initialize the step counter \n",
        "    step = 0\n",
        "    # 4 means that I am going to make 4 logs of the metrics when training\n",
        "    n_prints = int(len(train_loader)/4)\n",
        "\n",
        "    for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      \n",
        "      train_loss, train_accuracy = 0, 0\n",
        "      model.train()\n",
        "      for batch, data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        audios, videos, labels = data\n",
        "        loss, accuracy = train_batch(audios, videos, labels, model, optimizer, criterion, accuracy_fn)\n",
        "        train_loss += loss.item()\n",
        "        train_accuracy += accuracy\n",
        "\n",
        "        # Report metrics every n_prints batch \n",
        "        if batch % n_prints == n_prints-1:\n",
        "            train_log(train_loss/(batch+1), train_accuracy/(batch+1), step, batch)\n",
        "            if losses:\n",
        "              losses.append(train_loss/(batch+1))\n",
        "            # print(f\"train loss: {loss:.3f} accuracy: {accuracy:.3f} [after {batch+1} batches]\")            \n",
        "            # Increment the step after logging\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **test loop**\n",
        "\n",
        "def test(model, test_loader, accuracy_fn, f1_score_fn, recall_fn, precision_fn):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    num_batches = len(test_loader)\n",
        "    test_accuracy, test_f1, test_recall, test_precision = 0, 0, 0, 0\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for audios, videos, labels in test_loader:\n",
        "            audios, videos, labels = audios.to(DEVICE), videos.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(audios, videos)\n",
        "            test_accuracy += accuracy_fn(outputs, labels.unsqueeze(1).float())\n",
        "            test_f1 += f1_score_fn(outputs, labels.unsqueeze(1).float())\n",
        "            test_recall += recall_fn(outputs, labels.unsqueeze(1).float())\n",
        "            test_precision += precision_fn(outputs, labels.unsqueeze(1).float())\n",
        "\n",
        "        # Average the metrics over all batches\n",
        "        test_accuracy /= num_batches\n",
        "        test_f1 /= num_batches\n",
        "        test_recall /= num_batches\n",
        "        test_precision /= num_batches\n",
        "\n",
        "        wandb.log({\"test_accuracy\": test_accuracy, \"test_f1\": test_f1,\n",
        "            \"test_recall\": test_recall, \"test_precision\": test_precision})\n",
        "        print(f\"test accuracy: {test_accuracy:.3f} recall: {test_recall:.3f} precision: {test_precision:.3f} f1: {test_f1:.3f} [after {num_batches} batches]\")\n",
        "\n",
        "    return test_accuracy, test_f1, test_recall, test_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make(config, fold=None):\n",
        "\n",
        "  # Make the data\n",
        "  train, test = get_data(slice=1, fold=fold)\n",
        "  train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "  test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "  # Creacion del modelo y lo movemos a la GPU o CPU\n",
        "  model = MultimodalModel(n_classes=1).to(DEVICE)\n",
        "  \n",
        "  # Funcion de perdida (loss)\n",
        "  criterion = nn.BCEWithLogitsLoss() # https://shivambaldha.medium.com/binary-classification-with-pytorch-85089b284940\n",
        "\n",
        "  # Algoritmos de optimizaciÃ³n del gradiente\n",
        "  # optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "  # Make metrics\n",
        "  accuracy_fn = BinaryAccuracy().to(DEVICE)\n",
        "  f1_score_fn = BinaryF1Score().to(DEVICE)\n",
        "  recall_fn = BinaryRecall().to(DEVICE)\n",
        "  precision_fn = BinaryPrecision().to(DEVICE)\n",
        "\n",
        "  EPOCHS = config.epochs\n",
        "\n",
        "  return model, train_loader, test_loader, criterion, optimizer, accuracy_fn, f1_score_fn, recall_fn, precision_fn, EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_pipeline(num, sweep_id, sweep_run_name, hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    run_name = f'{sweep_run_name}--{num}'\n",
        "    with wandb.init(config=hyperparameters, group=sweep_id, job_type=sweep_run_name, name=run_name, reinit=True):\n",
        "        # access all HPs through wandb.config, so logging matches execution!\n",
        "        config = wandb.config\n",
        "\n",
        "        # make the model, data, and optimization problem\n",
        "        model, train_loader, test_loader, criterion, optimizer, accuracy_fn, f1_score_fn, recall_fn, precision_fn, epochs = make(config, num)\n",
        "        # print(model)\n",
        "\n",
        "        # and use them to train the model\n",
        "        train(model, train_loader, criterion, optimizer, accuracy_fn, epochs)\n",
        "            \n",
        "        # get metrics of the model    \n",
        "        test_accuracy, test_f1, test_recall, test_precision = test(model, test_loader, accuracy_fn, f1_score_fn, recall_fn, precision_fn)\n",
        "\n",
        "    return test_accuracy, test_f1, test_recall, test_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reset_wandb_env():\n",
        "    exclude = {\n",
        "        \"WANDB_PROJECT\",\n",
        "        \"WANDB_ENTITY\",\n",
        "        \"WANDB_API_KEY\",\n",
        "    }\n",
        "    for key in os.environ.keys():\n",
        "        if key.startswith(\"WANDB_\") and key not in exclude:\n",
        "            del os.environ[key]\n",
        "\n",
        "def cross_validate(config):\n",
        "\n",
        "    sweep_run = wandb.init(config=config, project=\"dip-project\") # Inicio el sweep\n",
        "    sweep_id = sweep_run.sweep_id or \"unknown\" # Agarro el id del sweep\n",
        "    sweep_url = sweep_run.get_sweep_url() # Agarro el url del sweep\n",
        "    project_url = sweep_run.get_project_url() # Agarro la url del proyecto del sweep\n",
        "    sweep_group_url = f'{project_url}/groups/{sweep_id}' # Armo un string con la url del\n",
        "    # proyecto y el id del sweep, para poder agrupar supongo \n",
        "    sweep_run.notes = sweep_group_url # Asigno en las notas del sweep la ruta que acabe\n",
        "    # de crear\n",
        "    sweep_run.save() # Guardo el sweep con las rutas y esas cosas hechas anteriormente\n",
        "    sweep_run_name = sweep_run.name or sweep_run.id or \"unknown_2\" # Armo el string con el\n",
        "    # nombre del run del sweep\n",
        "    sweep_run_id = sweep_run.id # Consigo el id del run del sweep\n",
        "    sweep_run.finish() # Puaso el sweep\n",
        "    wandb.sdk.wandb_setup._setup(_reset=True) #  resets the settings which are set during the\n",
        "    # sweep run initialization. This is crucial and acts as a workaround because it resets the\n",
        "    # settings which causes the new run in a sweep to use the same run ID and settings upon it's initialization.\n",
        "\n",
        "    # Diccionario para guardar las mÃ©tricas de cada run\n",
        "    metrics = {\n",
        "        \"test_accuracy\": [],\n",
        "        \"test_recall\": [],\n",
        "        \"test_precision\": [],\n",
        "        \"test_f1_score\": []\n",
        "    }\n",
        "\n",
        "    for fold in range(1, 8):\n",
        "\n",
        "        reset_wandb_env() # Reinicio las variables de entorno en cada run\n",
        "\n",
        "        # Entreno y valido cada run pasandole el sweep y la config\n",
        "        test_accuracy, test_f1, test_recall, test_precision = model_pipeline(\n",
        "            sweep_id=sweep_id, num=fold,\n",
        "            sweep_run_name=sweep_run_name,\n",
        "            hyperparameters=dict(sweep_run.config)\n",
        "        )\n",
        "\n",
        "        # Agrego las mÃ©tricas del run actual\n",
        "        metrics[\"test_accuracy\"].append(test_accuracy.cpu())\n",
        "        metrics[\"test_recall\"].append(test_f1.cpu())\n",
        "        metrics[\"test_precision\"].append(test_recall.cpu())\n",
        "        metrics[\"test_f1_score\"].append(test_precision.cpu())\n",
        "\n",
        "    # resume the sweep run\n",
        "    sweep_run = wandb.init(id=sweep_run_id, resume=\"must\")\n",
        "\n",
        "    # Log metrics to sweep run\n",
        "    for metric, values in metrics.items():\n",
        "        avg_value = np.mean(values)\n",
        "        std_value = np.std(values)\n",
        "        # Average of each metric over all the folds in an experiment\n",
        "        sweep_run.log({f\"{metric}_mean\": avg_value, f\"{metric}_std\": std_value}) \n",
        "        print(f'{metric.capitalize()}:')\n",
        "        print(f'  Average: {avg_value:.2f}% (+/- {std_value:.2f}%)')\n",
        "        for fold, value in enumerate(values):\n",
        "            sweep_run.log({\n",
        "                f'fold_{metric}': value,\n",
        "                'fold': fold+1\n",
        "            })\n",
        "\n",
        "    sweep_run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'sweep-gkfold',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'test_accuracy_mean'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10, 15, 20]},\n",
        "        'batch_size': {'distribution': 'q_log_uniform_values',\n",
        "                               'max': 256,\n",
        "                               'min': 8,\n",
        "                               'q': 8},\n",
        "        'learning_rate': {'distribution': 'uniform',\n",
        "                            'max': 0.1,\n",
        "                            'min': 0.00001},\n",
        "        'dataset': {'value': 'Parkison'}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='ai1-project')\n",
        "wandb.agent(sweep_id, function=cross_validate)\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
