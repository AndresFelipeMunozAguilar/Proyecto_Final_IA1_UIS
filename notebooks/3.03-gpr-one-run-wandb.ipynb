{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjq5aYppPwR"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "# <font color='#ECA702'>**Análisis Multimodal de Síntomas en la Enfermedad de Parkinson**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "89BxmLrSpPwT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "#@title **Importar librerías**\n",
        "\n",
        "# Manipulación de datos\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Pytorch essentials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pytorch metrics\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "\n",
        "# Pytorch essentials for datasets.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Utils\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm\n",
        "# from datetime import datetime\n",
        "# import pytz\n",
        "# import copy\n",
        "\n",
        "# wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import GroupKFold\n",
        "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kyzPDeVpPwU",
        "outputId": "942fc83c-f610-4bdb-a882-65fa126bc7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1XRruCPRodR4OhRCRZYKsyODqa_GuBkfC/data_parkinson\n"
          ]
        }
      ],
      "source": [
        "#@title **Cargamos los datos desde Drive**\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/data_parkinson')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqX7Xwgd-TuU",
        "outputId": "d7ffd662-8297-48d4-d8a6-13cab5955389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "\u001b[0m\u001b[01;34mAudioVisualData_v7\u001b[0m/  data.csv\n"
          ]
        }
      ],
      "source": [
        "#@title **Extraer el zip**\n",
        "\n",
        "os.mkdir('../../../../data')\n",
        "os.popen('cp data.csv ../../../../data/data.csv')\n",
        "target_path = os.path.join('../../../../data') # Aquí es donde van a quedar los datos\n",
        "# deben quedar fuera de drive para poder acceder a ellos 'localmente'\n",
        "\n",
        "# Unzip the downloaded files\n",
        "zip_file_path = os.path.join('AudioVisualData_v7.zip')\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_path)\n",
        "\n",
        "# # Me muevo hasta donde quedaron los datos\n",
        "%cd ../../../../data\n",
        "# Verifico\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI_cmB-BpPwU"
      },
      "source": [
        "## <font color='#4C5FDA'>**Procesamiento del dataset**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0Bee8VqyYi"
      },
      "source": [
        "### <font color='#52F17F'>**Creación del Dataset Pandas auxiliar**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "2mB_EpmMqxm-",
        "outputId": "44908149-391d-4822-a101-05fcfa878226"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio_path</th>\n",
              "      <th>frames_path</th>\n",
              "      <th>label</th>\n",
              "      <th>flatten_log_mel_spectogram</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>patient</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.66636657714844, -19.715749740600586, -24....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-61.8838005065918, -28.279430389404297, -28.8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-59.091400146484375, -25.001014709472656, -28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-60.556915283203125, -20.145050048828125, -20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2</th>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/audio/C2-...</td>\n",
              "      <td>AudioVisualData_v7/Control/C2/Vowels/frames/C2...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-69.82708740234375, -20.313282012939453, -26....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                audio_path  \\\n",
              "patient                                                      \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/audio/C2-...   \n",
              "\n",
              "                                               frames_path  label  \\\n",
              "patient                                                             \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "C2       AudioVisualData_v7/Control/C2/Vowels/frames/C2...    0.0   \n",
              "\n",
              "                                flatten_log_mel_spectogram  \n",
              "patient                                                     \n",
              "C2       [-60.66636657714844, -19.715749740600586, -24....  \n",
              "C2       [-61.8838005065918, -28.279430389404297, -28.8...  \n",
              "C2       [-59.091400146484375, -25.001014709472656, -28...  \n",
              "C2       [-60.556915283203125, -20.145050048828125, -20...  \n",
              "C2       [-69.82708740234375, -20.313282012939453, -26....  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"data.csv\", index_col='patient')\n",
        "\n",
        "def string2array(string: str):\n",
        "  \"\"\" Convierte un string con datos a numpy array \"\"\"\n",
        "  string = string.strip('[]')\n",
        "  return np.fromstring(string, sep=',')\n",
        "\n",
        "data['flatten_log_mel_spectogram'] = data['flatten_log_mel_spectogram'].apply(string2array)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_gRyCKHpPwZ"
      },
      "source": [
        "### <font color='#52F17F'>**Creación del Dataset PyTorch**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_kEXiniXpPwV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Constante sacada de la media de cantidad de frames de todos los vídeos multiplicada por 0.1\n",
        "para tener el 10%\n",
        "\"\"\"\n",
        "\n",
        "FRAMES_PROMEDIO = 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8OL_3J7jpPwZ"
      },
      "outputs": [],
      "source": [
        "class PatientDataset(Dataset):\n",
        "  def __init__(self, dataframe, n_frames = FRAMES_PROMEDIO):\n",
        "    self.dataframe = dataframe\n",
        "    # self.transform = transform\n",
        "    self.n_frames = n_frames\n",
        "    # self.audios = sorted(os.listdir(audio_dir)) # Listamos todas las imagenes en la carpeta\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    \"\"\" Carga de la imagen \"\"\"\n",
        "\n",
        "    # Entramos a la carpeta y conseguimos el audio de la lista\n",
        "    # audio_path = os.path.join(self.audio_dir, self.audios[index])\n",
        "    # audio_path = os.path.join(self.dataframe['audio_path'][index])\n",
        "\n",
        "    # Leemos el audio\n",
        "    audio = self.dataframe['flatten_log_mel_spectogram'][index].astype(np.float32)\n",
        "    audio = audio.reshape((90, 10))\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0)\n",
        "\n",
        "    \"\"\" Carga de los frames \"\"\"\n",
        "\n",
        "    # Entramos a la carpeta y conseguimos los frames de la lista.\n",
        "      # La razon por la que uso la misma lista de audios es porque el audio y el vídeo\n",
        "      # tienen el mismo nombre, solo cambia la ruta de la carpeta y la extensión.\n",
        "    \"\"\" audios: AudioVisualData_v7/Control/C0/Phonemes/audio/C0-1-Pakata.mp3 \"\"\"\n",
        "    \"\"\" vídeo: AudioVisualData_v7/Control/C0/Phonemes/frames/C0-1-Pakata.mov \"\"\"\n",
        "    video_path = os.path.join(self.dataframe['frames_path'][index])\n",
        "    # video_path = os.path.join(self.video_dir, self.audios[index].replace(\".mp3\", \".mov\"))\n",
        "\n",
        "    frames_path = sorted(os.listdir(video_path))[:self.n_frames]\n",
        "    frames = []\n",
        "    for frame in frames_path:\n",
        "      frame_path = os.path.join(self.dataframe['frames_path'][index], frame)\n",
        "      # print(frame_path)\n",
        "      frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n",
        "      frame = cv2.resize(frame, (112, 112)) # Resize a 112x112\n",
        "      frame = frame / 255. # Normalizamos entre 0 y 1\n",
        "      frames.append(frame)\n",
        "\n",
        "    frames = np.expand_dims(np.stack(frames), axis=1).astype('float32')\n",
        "    frames = frames.transpose(1, 0, 2, 3)\n",
        "    frames = torch.from_numpy(frames)\n",
        "\n",
        "    \"\"\" Consiguiendo el label \"\"\"\n",
        "\n",
        "    # Para conseguir la etiqueta simplemente me tengo que fijar en la ruta. Si es 'Parkinson' 1\n",
        "    # sino 0.\n",
        "\n",
        "    label = 1 if 'Parkinson' in self.dataframe['audio_path'][index] else 0\n",
        "\n",
        "    return audio, frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hYyG5ZSpPwa",
        "outputId": "a20701d4-968c-41c6-d21e-478e6e27fd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1091\n",
            "torch.Size([1, 90, 10]) torch.Size([1, 14, 112, 112]) 0\n"
          ]
        }
      ],
      "source": [
        "complete_dataset = PatientDataset(data)\n",
        "print(complete_dataset.__len__())\n",
        "print(complete_dataset[0][0].shape, complete_dataset[0][1].shape , complete_dataset[0][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ed1FV_F-Tub"
      },
      "source": [
        "### <font color='#52F17F'>**Visualización de los datos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1gUnMuEs-Tuc"
      },
      "outputs": [],
      "source": [
        "def plot_frame_strip(frames, rows=2, cols=7):\n",
        "    \"\"\"\n",
        "    Hace un plot de la tira de frames en una matriz de imágenes.\n",
        "\n",
        "    Args:\n",
        "        frames (np.ndarray): Tensor de frames con forma (num_frames, height, width, channels).\n",
        "        rows (int): Número de filas en la tira de frames.\n",
        "        cols (int): Número de columnas en la tira de frames.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 6))\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            idx = i * cols + j\n",
        "            if idx < len(frames):\n",
        "                axes[i, j].imshow(frames[idx], cmap='gray')\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].set_title(f\"Frame: {idx}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91lout8e-Tud"
      },
      "outputs": [],
      "source": [
        "plot_frame_strip(complete_dataset[0][1].squeeze(0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M2ZUYD9n-Tud",
        "outputId": "4473ff0e-b9b5-4660-9192-3e8d4290fd22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 10)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complete_dataset[0][0].squeeze(0).numpy().reshape((90, 10)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s70qLbPo-Tue",
        "outputId": "d45ca1d5-f9bf-4e98-a60d-e3e519812cd0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "librosa.display.specshow(complete_dataset[0][0].squeeze(0).numpy(), y_axis='mel', x_axis='time', sr=48000)\n",
        "plt.colorbar(format=\"%+2.f dB\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f3z4R6jpPwb"
      },
      "source": [
        "### <font color='#52F17F'>**Partición de los datos**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer los datos para GroupKFold https://discuss.pytorch.org/t/custom-datatype-for-many-images-to-one-label/87629\n",
        "X = [i for i in range(len(data))]\n",
        "y = data['label'].values\n",
        "groups = data.index.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "División 1:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P7']\n",
            "  Pacientes en prueba: ['C5' 'P8']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 2:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P6' 'P8']\n",
            "  Pacientes en prueba: ['C4' 'P7']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 3:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P5' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C3' 'P6']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 4:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P4' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C2' 'P5']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 5:\n",
            "  Pacientes en entrenamiento: ['C0' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P1' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C1' 'P4']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 6:\n",
            "  Pacientes en entrenamiento: ['C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'P0' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C0' 'P1']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n",
            "División 7:\n",
            "  Pacientes en entrenamiento: ['C0' 'C1' 'C2' 'C3' 'C4' 'C5' 'P1' 'P4' 'P5' 'P6' 'P7' 'P8']\n",
            "  Pacientes en prueba: ['C6' 'P0']\n",
            "  Número de pacientes en entrenamiento: 12\n",
            "  Número de pacientes en prueba: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "groupk_folds = 7\n",
        "gkf = GroupKFold(n_splits=groupk_folds)\n",
        "# Realizar la validación cruzada por grupos\n",
        "for i, (train_index, test_index) in enumerate(gkf.split(X, y, groups), 1):\n",
        "    train_groups = groups[train_index]\n",
        "    test_groups = groups[test_index]\n",
        "\n",
        "    print(f\"División {i}:\")\n",
        "    print(\"  Pacientes en entrenamiento:\", np.unique(train_groups))\n",
        "    print(\"  Pacientes en prueba:\", np.unique(test_groups))\n",
        "    print(\"  Número de pacientes en entrenamiento:\", len(np.unique(train_groups)))\n",
        "    print(\"  Número de pacientes en prueba:\", len(np.unique(test_groups)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='#52F17F'>**Make's for wandb**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(train_index, test_index, data):\n",
        "    full_dataset = PatientDataset(data)\n",
        "\n",
        "    train_subset = torch.utils.data.Subset(full_dataset, train_index)\n",
        "    test_subset = torch.utils.data.Subset(full_dataset, test_index)\n",
        "    \n",
        "    return train_subset, test_subset\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = torch.optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVi02_rZpPwb"
      },
      "source": [
        "## <font color='#ECA702'>**Modelo CNN 3D**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irytALTWpPwb"
      },
      "source": [
        "### <font color='#52F17F'>**Creando el modelo**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conventional and convolutional neural network\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, kernels, input_size=[1, 14, 112, 112], classes=1):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv3d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv3d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.kernels = kernels\n",
        "\n",
        "        self.final_feature_size = self.calculate_final_feature_size()\n",
        "        # Adjust the fully connected layer to use the final feature size\n",
        "        self.fc = nn.Linear(self.final_feature_size, classes)\n",
        "        # self.fc = nn.Linear(3 * 28 * 28 * kernels[-1], classes)\n",
        "    \n",
        "    def calculate_final_feature_size(self):\n",
        "        size_d, size_h, size_w = self.input_size[1:]\n",
        "        size_d = self.conv_output_size(size_d, 5, 1, 2) // 2  # layer1 depth\n",
        "        size_h = self.conv_output_size(size_h, 5, 1, 2) // 2  # layer1 height\n",
        "        size_w = self.conv_output_size(size_w, 5, 1, 2) // 2  # layer1 width\n",
        "        \n",
        "        size_d = self.conv_output_size(size_d, 5, 1, 2) // 2  # layer2 depth\n",
        "        size_h = self.conv_output_size(size_h, 5, 1, 2) // 2  # layer2 height\n",
        "        size_w = self.conv_output_size(size_w, 5, 1, 2) // 2  # layer2 width\n",
        "        \n",
        "        return size_d * size_h * size_w * self.kernels[-1]\n",
        "\n",
        "    def conv_output_size(self, size, kernel_size, stride, padding):\n",
        "        return (size - kernel_size + 2 * padding) // stride + 1\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHuZqJhpPwc",
        "outputId": "5657d424-ff1d-4fd6-e23d-7b258f1d7f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada video: (torch.Size([1, 1, 14, 112, 112]), torch.float32)\n",
            "Salida: torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "# n_channels = 1\n",
        "# n_classes = 2\n",
        "# input_audio = complete_dataset[0][0].unsqueeze(0)\n",
        "input_video = complete_dataset[0][1].unsqueeze(0)\n",
        "# print(f\"Entrada audio: {input_audio.size(), input_audio.dtype}\")\n",
        "print(f\"Entrada video: {input_video.size(), input_video.dtype}\")\n",
        "\n",
        "model = ConvNet(kernels=[32, 64], classes=1)\n",
        "ouput = model(input_video)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMiRa6oFEd8B",
        "outputId": "2b43aca6-e25d-45e0-fa21-592b5d2a9906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrada video: (torch.Size([2, 1, 14, 112, 112]), torch.float32)\n",
            "Salida: torch.Size([2, 1])\n"
          ]
        }
      ],
      "source": [
        "# Test al modelo, para ver si nos entrega la salida esperada\n",
        "\n",
        "input_video = torch.rand([2, 1, 14, 112, 112])\n",
        "# input_audio = torch.rand([2, 1, 90, 10])\n",
        "\n",
        "# print(f\"Entrada audio: {input_audio.size(), input_audio.dtype}\")\n",
        "print(f\"Entrada video: {input_video.size(), input_video.dtype}\")\n",
        "\n",
        "model = ConvNet(kernels=[32, 64], classes=1)\n",
        "ouput = model(input_video)\n",
        "print(f\"Salida: {ouput.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1])\n"
          ]
        }
      ],
      "source": [
        "targets = torch.rand([2])\n",
        "print(targets.unsqueeze(1).size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Oqu4j5pPwd"
      },
      "source": [
        "### <font color='#52F17F'>**Entrenamiento del modelo**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "njKMV7L_pPwd"
      },
      "outputs": [],
      "source": [
        "# Hiperparametros\n",
        "LEARNING_RATE = 0.005\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    epochs=5,\n",
        "    classes=1,\n",
        "    features=[32, 64],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    optimizer = 'adam',\n",
        "    dataset=\"ParkinsonVideo\",\n",
        "    architecture=\"CNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make(config, train_index, test_index, data):\n",
        "    # Make the data\n",
        "    train, test = get_data(train_index, test_index, data)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = ConvNet(kernels=config.features, classes=config.classes).to(DEVICE)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = build_optimizer(model, config.optimizer, config.learning_rate)\n",
        "\n",
        "    # N-epochs to train\n",
        "    epochs = config.epochs\n",
        "\n",
        "    # Make metrics\n",
        "    metric_fn = BinaryAccuracy().to(DEVICE)\n",
        "    \n",
        "    return model, train_loader, test_loader, criterion, optimizer, metric_fn, epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='##6600CC'>**Training loop**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_log(loss, accuracy, step, current):\n",
        "    \"\"\" Log the metrics for the current batch into wandb\n",
        "\n",
        "    Args:\n",
        "        loss: the value of the loss at current batch\n",
        "        accuracy: the value of the accuracy at current batch\n",
        "        step: actual step\n",
        "        current: actual batch\n",
        "    \"\"\"\n",
        "\n",
        "    # Where the magic happens\n",
        "    # wandb.log({\"step\":step, \"loss\": loss, \"accuracy\": accuracy}, step=step)\n",
        "    wandb.log({\"step\":step, \"train_loss\": loss, \"train_accuracy\": accuracy})\n",
        "    print(f\"train loss: {loss:.3f} accuracy: {accuracy:.3f} [after {current} batches]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_batch(images, labels, model, optimizer, criterion, metrics_fn):\n",
        "    \"\"\"Train the model on a single bacth of the dataloader.\n",
        "\n",
        "    Args:\n",
        "        dataloader: an instance of `torch.utils.data.DataLoader`, containing the training data.\n",
        "        model: an instance of `torch.nn.Module`, the model to be trained.\n",
        "        optimizer: an instance of `torch.optim.Optimizer`, the optimizer used for training.\n",
        "        criterion: a callable, the loss function.\n",
        "        metrics_fn: a callable, the metrics function.\n",
        "\n",
        "    Returns:\n",
        "        loss: the value of the loss at current batch\n",
        "        accuracy: the value of the accuracy at current batch\n",
        "    \"\"\"\n",
        "\n",
        "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "    accuracy = metrics_fn(outputs, labels.unsqueeze(1).float())\n",
        "    \n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer, metric_fn, step):\n",
        "\n",
        "    model.train()\n",
        "    n_prints = int(len(loader)/4)\n",
        "    \n",
        "    # Run training and track with wandb\n",
        "    for batch, (_, images, labels) in enumerate(loader):\n",
        "\n",
        "        loss, accuracy = train_batch(images, labels, model, optimizer, criterion, metric_fn)\n",
        "\n",
        "        # Report metrics every n_prints batch\n",
        "        if batch % n_prints == n_prints-1:\n",
        "            loss, current = loss.item(), batch\n",
        "            # print(step)\n",
        "            train_log(loss, accuracy, step, current)\n",
        "            # Increment the step after logging\n",
        "            step += 1\n",
        "    return step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='##6600CC'>**Evaluation loop**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, test_loader, loss_fn, metrics_fn, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    num_batches = len(test_loader)\n",
        "    val_loss, val_accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for _, images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            val_loss += loss_fn(outputs, labels.unsqueeze(1).float()).item()\n",
        "            val_accuracy += metrics_fn(outputs, labels.unsqueeze(1).float())\n",
        "\n",
        "    val_loss /= num_batches\n",
        "    val_accuracy /= num_batches\n",
        "        \n",
        "    # Log the evaluation metrics at the end of batches\n",
        "    wandb.log({\"epoch\": epoch+1, \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "    print(f\"val loss: {val_loss:.3f} accuracy: {val_accuracy:.3f} [after {num_batches} batches]\")\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='#52F17F'>**Train and watch your metrics on wandb.ai**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS_DIR = \"../models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters, train_index, test_index, data):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"ai-1-project\", config=hyperparameters, dir=MODELS_DIR):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer, metric_fn, epochs = make(config,\n",
        "                                                                                        train_index,\n",
        "                                                                                        test_index, \n",
        "                                                                                        data)\n",
        "      print(model)\n",
        "\n",
        "      # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "      wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "      # and use them to train the model\n",
        "\n",
        "      # Initialize the step counter \n",
        "      step = 0\n",
        "\n",
        "      for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        step = train(model, train_loader, criterion, optimizer, metric_fn, step=step)    \n",
        "        # and test its performance per epoch    \n",
        "        images = test(model, test_loader, criterion, metric_fn, epoch=t)\n",
        "      # Save the model in the exchangeable ONNX format\n",
        "      torch.onnx.export(model, images,\"model.onnx\")\n",
        "      wandb.save(\"model.onnx\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>../models/wandb/run-20240531_171059-f8wqrii8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aiuis/ai-1-project/runs/f8wqrii8' target=\"_blank\">dainty-violet-20</a></strong> to <a href='https://wandb.ai/aiuis/ai-1-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aiuis/ai-1-project' target=\"_blank\">https://wandb.ai/aiuis/ai-1-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aiuis/ai-1-project/runs/f8wqrii8' target=\"_blank\">https://wandb.ai/aiuis/ai-1-project/runs/f8wqrii8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv3d(1, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv3d(32, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=150528, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.693 accuracy: 0.750 [after 57 batches]\n",
            "train loss: 0.696 accuracy: 0.250 [after 115 batches]\n",
            "train loss: 0.699 accuracy: 0.750 [after 173 batches]\n",
            "train loss: 0.693 accuracy: 0.500 [after 231 batches]\n",
            "eval loss: 0.693 accuracy: 0.506 [after 39 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.684 accuracy: 0.000 [after 57 batches]\n",
            "train loss: 0.693 accuracy: 0.500 [after 115 batches]\n",
            "train loss: 0.778 accuracy: 0.500 [after 173 batches]\n",
            "train loss: 0.326 accuracy: 1.000 [after 231 batches]\n",
            "eval loss: 0.353 accuracy: 0.517 [after 39 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.118 accuracy: 1.000 [after 57 batches]\n",
            "train loss: 0.176 accuracy: 1.000 [after 115 batches]\n",
            "train loss: 0.115 accuracy: 1.000 [after 173 batches]\n",
            "train loss: 0.031 accuracy: 1.000 [after 231 batches]\n",
            "eval loss: 2.635 accuracy: 0.504 [after 39 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.001 accuracy: 1.000 [after 57 batches]\n",
            "train loss: 0.210 accuracy: 1.000 [after 115 batches]\n",
            "train loss: 0.003 accuracy: 1.000 [after 173 batches]\n",
            "train loss: 0.002 accuracy: 1.000 [after 231 batches]\n",
            "eval loss: 2.813 accuracy: 0.500 [after 39 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.214 accuracy: 1.000 [after 57 batches]\n",
            "train loss: 0.000 accuracy: 1.000 [after 115 batches]\n",
            "train loss: 0.223 accuracy: 1.000 [after 173 batches]\n",
            "train loss: 0.003 accuracy: 1.000 [after 231 batches]\n",
            "eval loss: 3.137 accuracy: 0.502 [after 39 batches]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▆▃▆▅▁▅▅█████████████</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>eval_accuracy</td><td>▄█▃▁▂</td></tr><tr><td>eval_loss</td><td>▂▁▇▇█</td></tr><tr><td>loss</td><td>▇▇▇▇▇▇█▄▂▃▂▁▁▃▁▁▃▁▃▁</td></tr><tr><td>step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>1.0</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>eval_accuracy</td><td>0.50214</td></tr><tr><td>eval_loss</td><td>3.13745</td></tr><tr><td>loss</td><td>0.00298</td></tr><tr><td>step</td><td>19</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dainty-violet-20</strong> at: <a href='https://wandb.ai/aiuis/ai-1-project/runs/f8wqrii8' target=\"_blank\">https://wandb.ai/aiuis/ai-1-project/runs/f8wqrii8</a><br/> View project at: <a href='https://wandb.ai/aiuis/ai-1-project' target=\"_blank\">https://wandb.ai/aiuis/ai-1-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>../models/wandb/run-20240531_171059-f8wqrii8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config, train_index, test_index, data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
